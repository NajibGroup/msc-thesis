\chapter{Evaluation of the Prototype}
\label{chap:evaluation-of-the-prototype}
In this chapter I present the measurements of various system functions and the runtime characteristics of the prototype framework.

Due to the underlying approached, the presented framework has its limitations and tradeoffs. Processing one file at a time makes the approach incremental with file-level granularity, potentially saving time on the whole, but takes a large amount of time integrating the parser into the system. The approach also requires less memory during the parsing phase, but takes more time once every file was processed and the connecting phase takes place.

\section{Benchmarking Environment}
In order to make sure that the measurements are reproduceable, and are not affected by user input or other environmental events, the measurements were carried out in the cloud. In this section I detail the hardware and software configurations for the benchmarks.

\subsection{Virtual Machine Configuration}
The benchmarks were carried out in a Microsoft Azure virtual machine~\cite{azure-vm}. Since the approach utilizes a persisted graph database with in-memory caching, I have chosen a configuration with moderate amount of memory for a server and high IO performance.

The D-series virtual machines were designed with data-intensive use-cases in mind, like Big Data and Analytics.~\cite{d-series} The virtual machine instance was located in the West Europe region.

The \emph{Standard DS3\_v2} configuration consists of the following:
\begin{itemize}[topsep=0pt]
  \item 4 CPU cores (Intel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHz --- \emph{reported})
  \item 14 GB memory
  \item 8 data disks
  \item 12800 maximum IOPS
  \item 28 GB local SSD
\end{itemize}

\subsection{Software Configuration}
The results of the benchmarks can also be affected by the software configuration. I have selected the preconfigured \emph{Ubuntu Server} virtual machine with the following properties and modifications:

\begin{itemize}[topsep=0pt]
  \item Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0-42-generic x86\_64)
  \item Oracle Java(TM) SE Runtime Environment (build 1.8.0\_111-b14)
  \item Java Virtual Machine with 2 GB minimum and 12 GB maximum heap space
  \item bash benchmarking script with curl
\end{itemize}

\subsection{Framework Dependencies}
The prototype of the framework was based on changing and rapidly developed dependencies. Both Neo4j and Shift had version and API changes, so I chose to freeze the versions at a working state and use them for the measurements.

Neo4j was freezed at the first released 3.0 version: 3.0.0. At the time of writing this paper it is at version 3.0.6, with 3.1 being available as a beta. Shift was freezed at 2.2.0 with custom modifications --- accepted and merged into later versions.


\section{Benchmark Cases}
In this section I iterate over the various benchmark cases, present them in detail, introduce the results of the measurements and evaluate the results.

\subsection{Graph Database Initialization}
The framework uses the Neo4j server in embedded mode (instead of a standalone configuration). This means that the server is started with the framework and at the first usage it needs initialization.

While the following benchmarks are prepared with initialized databases, I have measured the time required to prepare an empty database. This is measured by deleting the database folder, restarting the application and executing a simple query counting the nodes.

The database requires 1 second to initialize (936, 945, 1010, 1019, 1022, and 1042 milliseconds in the 6 runs, respectively).

\subsection{Source to Graph Transformation}
After the database has been initialized, it can receive content. This transformation process is one of the most time-consuming workflows. Here the source code of the file is read from the disk or the memory, transferred to the server. It is then parsed using the Shift parser, extended with the scope analyzer and stored in the database while iterating over every node in it.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{import-steps.pdf}
  \caption{The characteristics of the import steps.}
  \label{fig:import-steps}
\end{figure}

\Cref{fig:import-steps} shows the characteristics of these three steps. Since even the longest source codes can be written in one line, instead of the source lines of code I have chosen the number of nodes in the transformed subgraph as the horizontal axis. The vertical axis represents the time (in milliseconds) required to perform the given transformation step. Note that both axes are logarithmic.

In order to present an accurate and wide-range measurement, I have selected the repository of the Tresorit web client~\cite{tresorit-webclient}. The current version of this repository contains 780 JavaScript files, with 75\,907 lines of actual code in total. This results in 8\,437\,838 graph nodes.

Since the source code contains language elements not yet standardized, I transpiled the files before they are parsed by the Shift parser. This step is not calculated in the benchmark. The resulting files are then imported into the database one-by-one, sequentially, thus the parallel optimizations are not present in this measurement.

Based on~\Cref{fig:import-steps} the time requirement of the parsing and scope analysis steps are negligible compared to the third step. Iterating and storing the elements of the ASG in the graph database shows polinomial correlation with the size of the graph. It is also visible that most of the files are parsed under one second, which makes this approach useable in real-life usage.

These results are based on two separate, sequential, full import of the source code repository.

\subsection{Connecting the Subgraphs}
% TODO write or hide

\subsection{Dead Code Search}
% TODO write

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{deadcode-search.pdf}
  \caption{The characteristics of dead code search.}
  \label{fig:deadcode-search}
\end{figure}

\subsection{ASG to CFG Transformation}
In order to measure the characteristics of the CFG transformation, I have imported the source files of the Tresorit web client one-by-one and executed the CFG transformation queries for that one file.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{build-cfg.pdf}
  \caption{The characteristics of building the CFG.}
  \label{fig:build-cfg}
\end{figure}

\Cref{fig:build-cfg} shows that the measurements are clustered into three parts. There are at least two explanations for this:

\begin{itemize}[topsep=0pt]
  \item The number of CFG transformations implemented in the prototype framework is low. Since there are measurements in every cluster for a great amount of model sizes, it is possible that the composition of the files are different in each cluster.

  This would mean that the top cluster --- implementing business logic --- contains more nodes to transform, while the files in the bottom cluster --- mostly describing interfaces and proxies --- contain less.

  \item The transformation is executed in a parallel manner. If two transformations cause a deadlock in the database, they are canceled and tried again later. It is also possible that subgraphs containing more transformable nodes by the framework are processed slower. The slower the transformations are, the more deadlocks may happen, resulting in slower overall performance.
\end{itemize}

It is also unexpected to have the top two clusters show no correlation with the size of the resulting graph size. This phenomenon can be explained with the reasons above or with the way Neo4j executes the declarative transformations.

Since the CFG transformation in the framework is far from finished, deeper examination of the results is subject to future work.


\section{Threats to Validity}
\label{sect:evaluation-threats}
Although I carefully designed and executed each measurement, there might have been factors beyond control that influence the results yielded. In this section, I try to list the possible mistakes and also discuss the steps taken to mitigate their effects.

\subsection{Benchmarking in the Cloud} As a multiple access system, the virtual servers in the cloud can be easily affected by neighboring virtual machines using the same resources. The virtual machine manager can also limit the usage of these resources, if the machines disturb other ones. One can neither control the resources assigned to the machines, nor influence their precise geolocation and connections.

My mitigation strategy is to run the benchmarks multiple times and treat their median as the correct value.

\subsection{Methodological Mistakes} It is possible that I made mistakes while implementing the approach. It may not adhere to the specification correctly, perform the transformations correctly or measure correctly.

To check the validity of the results, I checked the results manually and with others tools.

\subsection{Measurement Mistakes} I may have committed mistakes in the measurements or was led to the wrong conclusions evaluating these mistakes.
